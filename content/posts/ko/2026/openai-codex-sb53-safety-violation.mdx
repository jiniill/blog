---
title: "OpenAI GPT-5.3-Codex, 캘리포니아 AI 안전법 위반 의혹 — 감시단체가 고위험 모델 안전조치 미이행을 지적하다"
slug: "openai-codex-sb53-safety-violation"
description: "감시단체 Midas Project가 OpenAI의 GPT-5.3-Codex가 캘리포니아 SB 53 AI 안전법상 요구되는 고위험 모델 안전조치를 이행하지 않았다고 주장했다. OpenAI는 반박 중이나, 안전 연구자들은 위반이 명백하다는 입장이다."
date: 2026-02-19
published: true
tags:
  - "AI"
  - "AI 안전"
  - "OpenAI"
  - "SB 53"
  - "규제"
author: "Snigdha Gairola"
sourceUrl: "https://sg.finance.yahoo.com/news/openais-gpt-5-3-codex-123124816.html"
sourceTitle: "OpenAI's GPT-5.3-Codex Faces California AI Safety Law Scrutiny As Watchdog Alleges High-Risk Violations"
references: []
---

OpenAI가 자체 안전 보고서에서 안전조치가 "기준에 미달"한다고 인정한 모델을 그대로 출시했다. 감시단체 **Midas Project**가 이를 캘리포니아 AI 안전법(**SB 53**) 위반으로 지목하면서, OpenAI의 안전 거버넌스가 다시 도마 위에 올랐다.

---

## 무슨 일이 있었나

**Midas Project**는 지난주 OpenAI의 최신 코딩 모델 **GPT-5.3-Codex**가 SB 53[^sb53]이 요구하는 안전조치를 갖추지 않은 채 출시됐다고 주장했다. 쟁점의 핵심은 OpenAI가 스스로 공개한 안전 보고서다. 해당 보고서 29페이지에는 현재 적용된 안전조치가 자체 안전 프레임워크의 요구 기준인 "Safeguards Report"에 **적합하지 않다**고 기술되어 있다.

GPT-5.3-Codex는 OpenAI가 AI 코딩 분야에서 주도권을 되찾기 위해 내놓은 모델로, 자체 벤치마크 기준 이전 모델과 경쟁사 대비 코딩 성능이 우위에 있다고 OpenAI는 밝힌 바 있다.

---

## 양측의 입장

### OpenAI의 반박

OpenAI 대변인은 Fortune과의 인터뷰에서 GPT-5.3-Codex가 **전체 테스트 및 거버넌스 절차**를 완료했으며, 프록시 평가와 내부 전문가(Safety Advisory Group 포함)의 판단을 통해 **장거리 자율 행동 능력이 확인되지 않았다**고 밝혔다.

### 감시단체와 안전 연구자들의 반론

Midas Project 설립자 **Tyler Johnston**은 SB 53의 요구 수준 자체가 매우 낮다는 점을 지적했다.

> SB 53이 요구하는 건 기본적으로 자율적인 안전 계획을 수립하고, 그에 대해 솔직하게 소통하며, 필요하면 수정하되 위반하거나 거짓말하지 않는 것뿐이다.

**Encode**의 안전 연구자 **Nathan Calvin** 역시 관련 문서를 검토한 뒤, 위반 여부가 모호하지 않다는 입장을 밝혔다.

---

## 한편, OpenAI의 성장은 계속된다

이번 논란과 별개로 OpenAI의 사업 지표는 상승세다.

| 지표 | 수치 |
|------|------|
| ChatGPT 월간 성장률 | **10% 이상** (CEO Sam Altman, 내부 슬랙 메시지) |
| GPT-5.3-Codex 출시 후 Codex 사용량 증가 | **약 50%** |

Altman은 같은 주에 업데이트된 Chat 모델 출시도 예고했다. 한편 OpenAI는 Anthropic의 슈퍼볼 광고에 대해 ChatGPT 내 광고 게재를 비판하며 반격했는데, Altman은 해당 광고를 "기만적"이라고 표현했다. 다만 관계자에 따르면 OpenAI도 응답 하단에 명확히 표시된 광고를 테스트할 계획이 있는 것으로 알려졌다.

---

## 왜 주목할 만한가

이번 사안의 핵심은 기술적 안전성 그 자체보다 **자체 안전 프레임워크와 실제 행동 사이의 괴리**다. OpenAI는 고위험으로 분류된 모델에 대해 자체적으로 설정한 안전 기준을 충족하지 못했다고 스스로 기술해놓고, 그 모델을 출시했다. SB 53은 기업에게 특정 안전 기준을 강제하지 않는다 — 기업이 **스스로 정한 기준을 지키라**고 요구할 뿐이다. 그 낮은 기준조차 지키지 않았다는 것이 감시단체의 주장이다.

캘리포니아 AI 안전법의 실질적 집행력이 시험대에 오른 첫 번째 사례가 될 수 있다.

[^sb53]: SB 53 — 캘리포니아주의 AI 안전법. 고위험 AI 모델을 출시하는 기업에게 자체 안전 계획 수립과 그에 따른 이행을 요구한다.
